global:
  ingress: 
    enabled: false

configs:
  cm:
    create: true
    url: https://argocd.tamhomelab.com
    # Enable Helm support in Kustomize
    kustomize.buildOptions: "--enable-helm"

    # Ignore Gateway API HTTPRoute parentRefs normalization
    resource.customizations.ignoreDifferences.gateway.networking.k8s.io_HTTPRoute: |
      jqPathExpressions:
      - '.spec.parentRefs[]? | select(.group == null or .kind == null)'
      jsonPointers:
      - /spec/parentRefs
    
    # Needed for App of Apps pattern sycn waves to work correctly
    # Without this, the root app is created and marked healthy before the childs apps are healthy
    # Making argoCD think the sync is complete when it is not and going to the next sync wave
    resource.customizations.health.argoproj.io_Application: |
      hs = {}
      hs.status = "Progressing"
      hs.message = ""
      if obj.status ~= nil then
        if obj.status.health ~= nil then
          hs.status = obj.status.health.status
          if obj.status.health.message ~= nil then
            hs.message = obj.status.health.message
          end
        end
      end
      return hs

  params:
    server.insecure: true

crds:
  install: true
  # -- Keep CRDs on chart uninstall
  keep: false

controller:
  metrics:
    enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  podLabels:
    # Cilium network policy labels
    netpol.cilium.io/egress-to-intra: "true"
    netpol.cilium.io/egress-to-kube-apiserver: "true"
    netpol.cilium.io/egress-to-kube-dns: "true"
    # netpol.cilium.io/ingress-from-host: "true"
    netpol.cilium.io/ingress-from-prometheus: "true"


# https://argo-cd.readthedocs.io/en/stable/developer-guide/architecture/components/#dex
dex:
  metrics:
    enabled: true
  resources:
    requests:
      cpu: 20m
      memory: 64Mi
    limits:
      cpu: 1000m
      memory: 256Mi
  podLabels:
    # Cilium network policy labels
    # don't use any external OIDC providers, so deny all ingress
    # maybe change this if added external OIDC provider in the future
    netpol.cilium.io/egress-to-kube-apiserver: "true"
    netpol.cilium.io/ingress-deny: "true"

redis:
  metrics:
    enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 64Mi
    limits:
      cpu: 2000m
      memory: 128Mi
  podLabels:
    # Cilium network policy labels
    netpol.cilium.io/egress-deny: "true"
    netpol.cilium.io/ingress-from-intra: "true"

server:
  metrics:
    enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 4000m
      memory: 256Mi
  podLabels:
    netpol.cilium.io/egress-to-intra: "true"
    netpol.cilium.io/egress-to-kube-apiserver: "true"
    netpol.cilium.io/egress-to-kube-dns: "true"
    # netpol.cilium.io/ingress-from-host: "true"
    netpol.cilium.io/ingress-from-ingress: "true"
    netpol.cilium.io/ingress-from-prometheus: "true"
    netpol.cilium.io/ingress-from-remote-node: "true"

repoServer:
  metrics:
    enabled: true
  containerSecurityContext:
    readOnlyRootFilesystem: true
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 4000m
      memory: 1024Mi
  podLabels:
    # Cilium network policy labels
    netpol.cilium.io/egress-to-intra: "true"
    netpol.cilium.io/egress-to-kube-dns: "true"
    netpol.cilium.io/egress-to-public-ips: "true"
    netpol.cilium.io/ingress-from-intra: "true"
    netpol.cilium.io/ingress-from-prometheus: "true"

applicationSet:
  metrics:
    enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 1000m
      memory: 512Mi
  podLabels:
    # Cilium network policy labels
    netpol.cilium.io/egress-to-intra: "true"
    netpol.cilium.io/egress-to-kube-apiserver: "true"
    netpol.cilium.io/egress-to-kube-dns: "true"
    netpol.cilium.io/ingress-from-prometheus: "true"